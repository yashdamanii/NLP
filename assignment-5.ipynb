{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e86cf721",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-07T17:39:40.336065Z",
     "iopub.status.busy": "2025-09-07T17:39:40.335668Z",
     "iopub.status.idle": "2025-09-07T17:40:18.422228Z",
     "shell.execute_reply": "2025-09-07T17:40:18.421398Z"
    },
    "papermill": {
     "duration": 38.093007,
     "end_time": "2025-09-07T17:40:18.423846",
     "exception": false,
     "start_time": "2025-09-07T17:39:40.330839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-07 17:39:59.818734: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757266800.059340      13 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757266800.132552      13 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import losses, evaluation\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "from datasets import Dataset\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, losses\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.similarity_functions import SimilarityFunction\n",
    "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
    "from sentence_transformers.training_args import SentenceTransformerTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cf614c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T17:40:18.431012Z",
     "iopub.status.busy": "2025-09-07T17:40:18.430014Z",
     "iopub.status.idle": "2025-09-07T17:40:18.731277Z",
     "shell.execute_reply": "2025-09-07T17:40:18.730017Z"
    },
    "papermill": {
     "duration": 0.306106,
     "end_time": "2025-09-07T17:40:18.732826",
     "exception": true,
     "start_time": "2025-09-07T17:40:18.426720",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/quora-question-pairs/train.csv.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13/3750890551.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/quora-question-pairs/train.csv.zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./train/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1293\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/quora-question-pairs/train.csv.zip'"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "\n",
    "with zipfile.ZipFile(\"/kaggle/input/quora-question-pairs/train.csv.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"./train/\")\n",
    "\n",
    "# Load and clean data\n",
    "df = pd.read_csv(\"/kaggle/working/train/train.csv\").dropna()\n",
    "df = df.rename(columns={'is_duplicate': 'label'})[['question1', 'question2', 'label']]\n",
    "\n",
    "# Split into train+val and test (80%/20%)\n",
    "train_val, test = model_selection.train_test_split(\n",
    "    df, test_size=0.2, random_state=42, stratify=df['label']\n",
    ")\n",
    "\n",
    "# Split train+val into train and val (75%/25% of train_val)\n",
    "train, val = model_selection.train_test_split(\n",
    "    train_val, test_size=0.25, random_state=42, stratify=train_val['label']\n",
    ")\n",
    "\n",
    "# Convert to Dataset objects\n",
    "from datasets import Dataset\n",
    "train_ds = Dataset.from_pandas(train.reset_index(drop=True))\n",
    "val_ds = Dataset.from_pandas(val.reset_index(drop=True))\n",
    "test_ds = Dataset.from_pandas(test.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697f9b7c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_f1(model, dataset, threshold=0.5, is_cross_encoder=False, batch_size=128):\n",
    "    q1, q2, labels = dataset['question1'], dataset['question2'], dataset['label']\n",
    "    n_samples = len(labels)\n",
    "    predictions = []\n",
    "    \n",
    "    if is_cross_encoder:\n",
    "        from sentence_transformers import CrossEncoder\n",
    "        if not isinstance(model, CrossEncoder):\n",
    "            model = CrossEncoder(model)\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in tqdm(range(0, n_samples, batch_size), desc=\"Evaluating Cross-Encoder\"):\n",
    "            batch_q1 = q1[i:i+batch_size]\n",
    "            batch_q2 = q2[i:i+batch_size]\n",
    "            batch_scores = model.predict(list(zip(batch_q1, batch_q2)))\n",
    "            batch_preds = (batch_scores >= threshold).astype(int)\n",
    "            predictions.extend(batch_preds)\n",
    "            \n",
    "    else:\n",
    "        # Process question1 and question2 separately in batches\n",
    "        emb1_list, emb2_list = [], []\n",
    "        \n",
    "        # Encode question1 in batches\n",
    "        for i in tqdm(range(0, n_samples, batch_size), desc=\"Encoding question1\"):\n",
    "            batch_q1 = q1[i:i+batch_size]\n",
    "            emb1_batch = model.encode(batch_q1, convert_to_tensor=False, show_progress_bar=False)\n",
    "            emb1_list.append(emb1_batch)\n",
    "        \n",
    "        # Encode question2 in batches\n",
    "        for i in tqdm(range(0, n_samples, batch_size), desc=\"Encoding question2\"):\n",
    "            batch_q2 = q2[i:i+batch_size]\n",
    "            emb2_batch = model.encode(batch_q2, convert_to_tensor=False, show_progress_bar=False)\n",
    "            emb2_list.append(emb2_batch)\n",
    "        \n",
    "        # Concatenate all batches\n",
    "        emb1 = np.concatenate(emb1_list, axis=0)\n",
    "        emb2 = np.concatenate(emb2_list, axis=0)\n",
    "        \n",
    "        # Normalize embeddings for cosine similarity\n",
    "        emb1_norm = emb1 / np.linalg.norm(emb1, axis=1, keepdims=True)\n",
    "        emb2_norm = emb2 / np.linalg.norm(emb2, axis=1, keepdims=True)\n",
    "        \n",
    "        # Compute cosine similarity element-wise (much more memory efficient)\n",
    "        cos_scores = np.sum(emb1_norm * emb2_norm, axis=1)\n",
    "        scores = (cos_scores + 1) / 2  # Convert from [-1,1] to [0,1]\n",
    "        predictions = (scores >= threshold).astype(int)\n",
    "    \n",
    "    return f1_score(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48684d83",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model_path\": \"microsoft/xtremedistil-l6-h256-uncased\",\n",
    "    \n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"train_batch_size\": 320,\n",
    "    \"eval_batch_size\": 320,\n",
    "    \"epochs\": 5,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \n",
    "    \"output_dir\": \".\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6af313",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer(config[\"model_path\"])\n",
    "val_f1 = evaluate_f1(model, val_ds)\n",
    "print(f\"Benchmark F1-Score: {val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a44188",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a more memory-efficient evaluator\n",
    "class F1Evaluator:\n",
    "    def __init__(self, dataloader, threshold=0.5):\n",
    "        self.dataloader = dataloader\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def __call__(self, model, output_path=None, epoch=-1, steps=-1):\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "        \n",
    "        for batch in tqdm(self.dataloader, desc=\"Evaluating\"):\n",
    "            features, labels = batch\n",
    "            emb1 = model.encode(features['question1'], convert_to_tensor=False)\n",
    "            emb2 = model.encode(features['question2'], convert_to_tensor=False)\n",
    "            \n",
    "            # Compute cosine similarity\n",
    "            emb1_norm = emb1 / np.linalg.norm(emb1, axis=1, keepdims=True)\n",
    "            emb2_norm = emb2 / np.linalg.norm(emb2, axis=1, keepdims=True)\n",
    "            cos_scores = np.sum(emb1_norm * emb2_norm, axis=1)\n",
    "            scores = (cos_scores + 1) / 2\n",
    "            \n",
    "            predictions = (scores >= self.threshold).astype(int)\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_predictions.extend(predictions)\n",
    "        \n",
    "        f1 = f1_score(all_labels, all_predictions)\n",
    "        return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe3ae39",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train with cosine similarity loss\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=config[\"output_dir\"],\n",
    "    \n",
    "    num_train_epochs=config[\"epochs\"],\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    per_device_train_batch_size=config[\"train_batch_size\"],\n",
    "    per_device_eval_batch_size=config[\"eval_batch_size\"],\n",
    "    \n",
    "    warmup_ratio=config[\"warmup_ratio\"],\n",
    "    \n",
    "    fp16=True,\n",
    "    \n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    loss=train_loss,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate on test set\n",
    "test_f1 = evaluate_f1(model, test_ds, batch_size=256)\n",
    "print(f\"CosineSimilarityLoss F1: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295d07e6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "contrastive_loss = losses.ContrastiveLoss(model=model)\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    loss=contrastive_loss,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "test_f1 = evaluate_f1(model, test_ds, batch_size=256)\n",
    "print(f\"ContrastiveLoss F1: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ae8fdf",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter only positive pairs for MNRL\n",
    "pos_indices = [i for i, label in enumerate(train_ds['label']) if label == 1]\n",
    "pos_train_ds = train_ds.select(pos_indices)\n",
    "\n",
    "mnrl_loss = losses.MultipleNegativesRankingLoss(model=model)\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=pos_train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    loss=mnrl_loss,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "test_f1 = evaluate_f1(model, test_ds, batch_size=256)\n",
    "print(f\"MNRL F1: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c9bee2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, classification_report\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "\n",
    "# Load tokenizer and model for sequence classification\n",
    "tokenizer = AutoTokenizer.from_pretrained(config[\"model_path\"])\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    config[\"model_path\"], num_labels=2\n",
    ")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    texts = [(q1, q2) for q1, q2 in zip(examples[\"question1\"], examples[\"question2\"])]\n",
    "    return tokenizer(\n",
    "        texts, \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_train = train_ds.map(tokenize_function, batched=True)\n",
    "tokenized_val = val_ds.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_ds.map(tokenize_function, batched=True)\n",
    "\n",
    "# Format datasets for PyTorch\n",
    "tokenized_train = tokenized_train.rename_column(\"label\", \"labels\")\n",
    "tokenized_val = tokenized_val.rename_column(\"label\", \"labels\")\n",
    "tokenized_test = tokenized_test.rename_column(\"label\", \"labels\")\n",
    "\n",
    "tokenized_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_val.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_test.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Metrics function for evaluation\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions, average='weighted')\n",
    "    recall = recall_score(labels, predictions, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'f1': f1,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config[\"output_dir\"],\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    per_device_train_batch_size=config[\"train_batch_size\"],\n",
    "    per_device_eval_batch_size=config[\"eval_batch_size\"],\n",
    "    num_train_epochs=config[\"epochs\"],\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    warmup_ratio=config[\"warmup_ratio\"],\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=500,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training cross-encoder model...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(config[\"output_dir\"])\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_results = trainer.evaluate(tokenized_test)\n",
    "print(f\"Test Results:\")\n",
    "print(f\"F1-Score: {test_results['eval_f1']:.4f}\")\n",
    "print(f\"Accuracy: {test_results['eval_accuracy']:.4f}\")\n",
    "print(f\"Precision: {test_results['eval_precision']:.4f}\")\n",
    "print(f\"Recall: {test_results['eval_recall']:.4f}\")\n",
    "\n",
    "# Get detailed predictions on test set\n",
    "test_predictions = trainer.predict(tokenized_test)\n",
    "predicted_labels = np.argmax(test_predictions.predictions, axis=1)\n",
    "true_labels = test_predictions.label_ids\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(true_labels, predicted_labels, target_names=['Not Duplicate', 'Duplicate']))\n",
    "\n",
    "# Calculate and display F1-score\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "print(f\"Final F1-Score on test set: {f1:.4f}\")\n",
    "\n",
    "# Example predictions\n",
    "print(\"\\nSample predictions from test set:\")\n",
    "sample_indices = np.random.choice(len(test), 5, replace=False)\n",
    "for i in sample_indices:\n",
    "    q1 = test.iloc[i][\"question1\"]\n",
    "    q2 = test.iloc[i][\"question2\"]\n",
    "    true_label = test.iloc[i][\"label\"]\n",
    "    pred_label = predicted_labels[i]\n",
    "    \n",
    "    print(f\"Q1: {q1}\")\n",
    "    print(f\"Q2: {q2}\")\n",
    "    print(f\"True: {'Duplicate' if true_label == 1 else 'Not Duplicate'}\")\n",
    "    print(f\"Pred: {'Duplicate' if pred_label == 1 else 'Not Duplicate'}\")\n",
    "    print(f\"Correct: {true_label == pred_label}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Additional analysis: Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Not Duplicate', 'Duplicate'],\n",
    "            yticklabels=['Not Duplicate', 'Duplicate'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 46.553119,
   "end_time": "2025-09-07T17:40:21.717305",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-07T17:39:35.164186",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
