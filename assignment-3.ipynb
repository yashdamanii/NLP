{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1d2fc57",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-07T17:04:40.751353Z",
     "iopub.status.busy": "2025-09-07T17:04:40.750914Z",
     "iopub.status.idle": "2025-09-07T17:06:00.200337Z",
     "shell.execute_reply": "2025-09-07T17:06:00.199341Z"
    },
    "papermill": {
     "duration": 79.455802,
     "end_time": "2025-09-07T17:06:00.201949",
     "exception": false,
     "start_time": "2025-09-07T17:04:40.746147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: /kaggle/input/imdb-dataset-csv/IMDB Dataset.csv\n",
      "Label distribution (full):\n",
      "label\n",
      "1    0.5\n",
      "0    0.5\n",
      "Name: proportion, dtype: float64\n",
      "Subset train size: 4050 | val size: 450\n",
      "Local HF model: NOT FOUND (will fallback to sklearn if needed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-07 17:05:04.393627: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757264704.610067      13 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757264704.685495      13 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer/model from: distilbert-base-uncased\n",
      "HF route unavailable. Falling back to sklearn baseline.\n",
      "Reason: OSError(\"We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.\\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\")\n",
      "Sklearn baseline â€” acc: 0.9060, prec: 0.9060, rec: 0.9060, f1: 0.9060\n",
      "Saved splits to /kaggle/working/\n"
     ]
    }
   ],
   "source": [
    "# === ONE-CELL IMDB SENTIMENT TRAINING + INFERENCE (Kaggle-friendly, offline-ready) ===\n",
    "# - Auto-finds IMDB CSV under /kaggle/input\n",
    "# - Fine-tunes a local HuggingFace model if available; else falls back to sklearn baseline\n",
    "# - Runs a pipeline on 10 test samples (readable labels: 'negative' / 'positive')\n",
    "\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# ---------------- IMDB CSV auto-discovery ----------------\n",
    "def find_imdb_csv():\n",
    "    base = Path(\"/kaggle/input\")\n",
    "    # Try canonical name first\n",
    "    hits = list(base.rglob(\"IMDB Dataset.csv\"))\n",
    "    if hits:\n",
    "        hits.sort(key=lambda p: -p.stat().st_size)\n",
    "        return hits[0]\n",
    "    # Fallback to imdb-ish CSVs\n",
    "    patterns = [\"*IMDB*Dataset*.csv\", \"*imdb*/*.csv\", \"*IMDB*/*.csv\", \"*imdb*.csv\"]\n",
    "    for pat in patterns:\n",
    "        hits = list(base.rglob(pat))\n",
    "        if hits:\n",
    "            hits.sort(key=lambda p: -p.stat().st_size)\n",
    "            return hits[0]\n",
    "    return None\n",
    "\n",
    "csv_path = find_imdb_csv()\n",
    "if not csv_path:\n",
    "    all_csvs = list(Path(\"/kaggle/input\").rglob(\"*.csv\"))\n",
    "    raise FileNotFoundError(\n",
    "        \"IMDB CSV not found under /kaggle/input.\\n\"\n",
    "        \"Attached CSVs:\\n  \" + \"\\n  \".join(map(str, all_csvs[:50]))\n",
    "    )\n",
    "\n",
    "print(\"Loading:\", csv_path)\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Expect 'review' and 'sentiment' columns\n",
    "if \"label\" not in df.columns:\n",
    "    if \"sentiment\" not in df.columns:\n",
    "        raise ValueError(\"Expected a 'sentiment' column with 'positive'/'negative'.\")\n",
    "    df[\"label\"] = df[\"sentiment\"].map({\"negative\": 0, \"positive\": 1})\n",
    "\n",
    "print(\"Label distribution (full):\")\n",
    "print(df[\"label\"].value_counts(normalize=True).round(4))\n",
    "\n",
    "# ---------------- Splits (matches your approach) ----------------\n",
    "trainval_df, test_df = train_test_split(\n",
    "    df, test_size=0.10, stratify=df[\"label\"], random_state=42\n",
    ")\n",
    "subset_frac = 0.10\n",
    "subset_df = trainval_df.sample(frac=subset_frac, random_state=42)\n",
    "train_subset, val_subset = train_test_split(\n",
    "    subset_df, test_size=0.10, stratify=subset_df[\"label\"], random_state=42\n",
    ")\n",
    "print(f\"Subset train size: {len(train_subset)} | val size: {len(val_subset)}\")\n",
    "\n",
    "# Full train/val (for a more serious run, like your later cell)\n",
    "train_full, val_full = train_test_split(\n",
    "    trainval_df, test_size=0.10, stratify=trainval_df[\"label\"], random_state=42\n",
    ")\n",
    "\n",
    "# ---------------- Local HF model discovery (no internet) ----------------\n",
    "def looks_like_hf_model_dir(d: Path) -> bool:\n",
    "    has_config = (d / \"config.json\").exists()\n",
    "    has_weights = (d / \"pytorch_model.bin\").exists() or (d / \"tf_model.h5\").exists()\n",
    "    has_tok = any((d / name).exists() for name in [\"tokenizer.json\", \"vocab.txt\"])\n",
    "    return d.is_dir() and has_config and has_weights and has_tok\n",
    "\n",
    "def find_local_hf_model(prefer=(\"distilbert\", \"bert\")) -> Path | None:\n",
    "    base = Path(\"/kaggle/input\")\n",
    "    if not base.exists():\n",
    "        return None\n",
    "    candidates = []\n",
    "    for d in base.rglob(\"*\"):\n",
    "        if looks_like_hf_model_dir(d):\n",
    "            name = d.name.lower()\n",
    "            score = 0\n",
    "            for i, key in enumerate(prefer):\n",
    "                if key in name:\n",
    "                    score += (len(prefer) - i)  # earlier keys higher\n",
    "            candidates.append((score, d))\n",
    "    if not candidates:\n",
    "        return None\n",
    "    candidates.sort(key=lambda x: x[0], reverse=True)\n",
    "    return candidates[0][1]\n",
    "\n",
    "local_model_dir = find_local_hf_model()\n",
    "print(\"Local HF model:\", local_model_dir if local_model_dir else \"NOT FOUND (will fallback to sklearn if needed)\")\n",
    "\n",
    "# ---------------- HF training utilities ----------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(labels, preds, average=\"macro\", zero_division=0)\n",
    "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n",
    "\n",
    "class HFTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=256):\n",
    "        self.texts = list(map(str, texts))\n",
    "        self.labels = list(labels)\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tok(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "def prepare_datasets(train_df, val_df, tokenizer, text_col=\"review\", label_col=\"label\", max_len=256):\n",
    "    train_ds = HFTextDataset(train_df[text_col], train_df[label_col], tokenizer, max_len)\n",
    "    val_ds   = HFTextDataset(val_df[text_col],   val_df[label_col],   tokenizer, max_len)\n",
    "    return train_ds, val_ds\n",
    "\n",
    "# ---------------- Try HF offline fine-tuning; else sklearn baseline ----------------\n",
    "did_hf = False\n",
    "try:\n",
    "    from transformers import (\n",
    "        AutoTokenizer, AutoModelForSequenceClassification,\n",
    "        Trainer, TrainingArguments, pipeline\n",
    "    )\n",
    "    if local_model_dir is not None:\n",
    "        local_only = True\n",
    "        model_name = str(local_model_dir)\n",
    "    else:\n",
    "        # If you KNOW you have internet, you can set local_only=False and a model name, e.g. 'distilbert-base-uncased'\n",
    "        local_only = True  # keep offline-safe by default\n",
    "        model_name = \"distilbert-base-uncased\"  # will fail offline if not cached\n",
    "\n",
    "    print(\"Loading tokenizer/model from:\", model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=local_only)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=2, local_files_only=local_only\n",
    "    ).to(device)\n",
    "\n",
    "    # ensure readable labels in model config\n",
    "    model.config.id2label = {0: \"negative\", 1: \"positive\"}\n",
    "    model.config.label2id = {\"negative\": 0, \"positive\": 1}\n",
    "\n",
    "    # datasets (use the fuller split for meaningful training, adjust epochs as needed)\n",
    "    train_ds_full, val_ds_full = prepare_datasets(train_full, val_full, tokenizer, max_len=256)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'/kaggle/working/results/{Path(model_name).name}_full',\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=5e-5,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=2,\n",
    "        report_to=[],\n",
    "    )\n",
    "\n",
    "    trainer_full = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds_full,\n",
    "        eval_dataset=val_ds_full,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer_full.train()\n",
    "    metrics = trainer_full.evaluate()\n",
    "    print(\"HF eval:\", metrics)\n",
    "    trainer_full.save_model()\n",
    "    print(\"Model saved to:\", training_args.output_dir)\n",
    "\n",
    "    # -------- Inference with pipeline on 10 samples --------\n",
    "    device_idx = 0 if torch.cuda.is_available() else -1\n",
    "    clf_pipeline = pipeline(\n",
    "        task=\"text-classification\",\n",
    "        model=trainer_full.model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device_idx,\n",
    "        batch_size=32,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    test_samples = test_df.sample(10, random_state=42)\n",
    "    reviews = test_samples[\"review\"].tolist()\n",
    "    true_labels = test_samples[\"label\"].tolist()\n",
    "\n",
    "    preds = clf_pipeline(reviews)\n",
    "\n",
    "    def to_int_label(p):\n",
    "        lbl = p[\"label\"].lower()\n",
    "        if lbl in model.config.label2id:\n",
    "            return model.config.label2id[lbl]\n",
    "        return 1 if lbl.endswith(\"1\") else 0\n",
    "\n",
    "    correct = 0\n",
    "    for i, (review, p, y) in enumerate(zip(reviews, preds, true_labels), 1):\n",
    "        pred_lbl = \"positive\" if to_int_label(p) == 1 else \"negative\"\n",
    "        true_lbl = \"positive\" if y == 1 else \"negative\"\n",
    "        correct += int(pred_lbl == true_lbl)\n",
    "        print(f\"\\nReview #{i}:\")\n",
    "        print(review[:300] + (\"...\" if len(review) > 300 else \"\"))\n",
    "        print(f\"Predicted: {pred_lbl} (score={p['score']:.4f}), True: {true_lbl}\")\n",
    "\n",
    "    acc = correct / len(true_labels)\n",
    "    print(f\"\\nPipeline sample accuracy: {acc:.2%}\")\n",
    "\n",
    "    did_hf = True\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"HF route unavailable. Falling back to sklearn baseline.\\nReason:\", repr(e))\n",
    "\n",
    "if not did_hf:\n",
    "    # --------- Sklearn Fallback (always works offline) ---------\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.pipeline import Pipeline\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(max_features=50000, ngram_range=(1,2))),\n",
    "        (\"clf\", LogisticRegression(max_iter=300))\n",
    "    ])\n",
    "    pipe.fit(train_full[\"review\"], train_full[\"label\"])\n",
    "    pred = pipe.predict(val_full[\"review\"])\n",
    "\n",
    "    acc = accuracy_score(val_full[\"label\"], pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(val_full[\"label\"], pred, average=\"macro\", zero_division=0)\n",
    "    print(f\"Sklearn baseline â€” acc: {acc:.4f}, prec: {prec:.4f}, rec: {rec:.4f}, f1: {f1:.4f}\")\n",
    "\n",
    "# Save splits for inspection/download\n",
    "out_dir = Path(\"/kaggle/working\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "train_subset.to_csv(out_dir / \"train_subset.csv\", index=False)\n",
    "val_subset.to_csv(out_dir / \"val_subset.csv\", index=False)\n",
    "test_df.to_csv(out_dir / \"test_full_10pct.csv\", index=False)\n",
    "print(\"Saved splits to /kaggle/working/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95aebd63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T17:06:00.208722Z",
     "iopub.status.busy": "2025-09-07T17:06:00.208346Z",
     "iopub.status.idle": "2025-09-07T17:06:05.160653Z",
     "shell.execute_reply": "2025-09-07T17:06:05.159240Z"
    },
    "papermill": {
     "duration": 4.957717,
     "end_time": "2025-09-07T17:06:05.162378",
     "exception": false,
     "start_time": "2025-09-07T17:06:00.204661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: /kaggle/input/imdb-dataset-csv/IMDB Dataset.csv\n",
      "Label distribution (full):\n",
      "label\n",
      "1    0.5\n",
      "0    0.5\n",
      "Name: proportion, dtype: float64\n",
      "Subset train size: 4050 | val size: 450\n",
      "Local HF model: NOT FOUND (will fallback to sklearn)\n",
      "Sklearn baseline accuracy: 0.8244\n",
      "Saved splits to /kaggle/working/\n"
     ]
    }
   ],
   "source": [
    "# Offline-friendly IMDB training: HuggingFace (local-only) with fallback to scikit-learn.\n",
    "# - Auto-finds IMDB CSV under /kaggle/input\n",
    "# - Forces transformers offline (no downloads)\n",
    "# - Loads a local model dir under /kaggle/input (e.g., *distilbert-base-uncased*)\n",
    "# - If no local model found, runs a TF-IDF + LogisticRegression baseline\n",
    "\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ---------------- IMDB CSV auto-discovery ----------------\n",
    "def find_imdb_csv():\n",
    "    base = Path(\"/kaggle/input\")\n",
    "    # Try canonical filename\n",
    "    hits = list(base.rglob(\"IMDB Dataset.csv\"))\n",
    "    if hits:\n",
    "        hits.sort(key=lambda p: -p.stat().st_size)\n",
    "        return hits[0]\n",
    "    # Fallback: anything imdb-ish\n",
    "    patterns = [\"*IMDB*Dataset*.csv\", \"*imdb*/*.csv\", \"*IMDB*/*.csv\", \"*imdb*.csv\"]\n",
    "    for pat in patterns:\n",
    "        hits = list(base.rglob(pat))\n",
    "        if hits:\n",
    "            hits.sort(key=lambda p: -p.stat().st_size)\n",
    "            return hits[0]\n",
    "    return None\n",
    "\n",
    "csv_path = find_imdb_csv()\n",
    "if not csv_path:\n",
    "    all_csvs = list(Path(\"/kaggle/input\").rglob(\"*.csv\"))\n",
    "    raise FileNotFoundError(\n",
    "        \"IMDB csv not found under /kaggle/input.\\n\"\n",
    "        \"Attached CSVs:\\n  \" + \"\\n  \".join(map(str, all_csvs[:50]))\n",
    "    )\n",
    "print(\"Loading:\", csv_path)\n",
    "\n",
    "# ---- Load & label ----\n",
    "df = pd.read_csv(csv_path)\n",
    "df[\"label\"] = df[\"sentiment\"].map({\"negative\": 0, \"positive\": 1})\n",
    "print(\"Label distribution (full):\")\n",
    "print(df[\"label\"].value_counts(normalize=True).round(4))\n",
    "\n",
    "# ---- Split like your original code ----\n",
    "trainval_df, test_df = train_test_split(\n",
    "    df, test_size=0.10, stratify=df[\"label\"], random_state=42\n",
    ")\n",
    "subset_frac = 0.10\n",
    "subset_df = trainval_df.sample(frac=subset_frac, random_state=42)\n",
    "train_subset, val_subset = train_test_split(\n",
    "    subset_df, test_size=0.10, stratify=subset_df[\"label\"], random_state=42\n",
    ")\n",
    "print(f\"Subset train size: {len(train_subset)} | val size: {len(val_subset)}\")\n",
    "\n",
    "# ---------------- Local HF model discovery (no internet) ----------------\n",
    "def looks_like_hf_model_dir(d: Path) -> bool:\n",
    "    # Require config + weights + tokenizer-ish file\n",
    "    has_config = (d / \"config.json\").exists()\n",
    "    has_weights = (d / \"pytorch_model.bin\").exists() or (d / \"tf_model.h5\").exists()\n",
    "    has_tok = any((d / name).exists() for name in [\"tokenizer.json\", \"vocab.txt\"])\n",
    "    return d.is_dir() and has_config and has_weights and has_tok\n",
    "\n",
    "def find_local_hf_model(prefer=(\"distilbert\", \"bert\")) -> Path | None:\n",
    "    base = Path(\"/kaggle/input\")\n",
    "    if not base.exists(): \n",
    "        return None\n",
    "    # Score directories that look like HF model repos\n",
    "    candidates = []\n",
    "    for d in base.rglob(\"*\"):\n",
    "        if d.is_dir() and looks_like_hf_model_dir(d):\n",
    "            name = d.name.lower()\n",
    "            score = 0\n",
    "            for i, key in enumerate(prefer):\n",
    "                if key in name:\n",
    "                    score += (len(prefer) - i)  # prefer earlier keys\n",
    "            # Prefer smaller models (distilbert) over bert if both exist\n",
    "            candidates.append((score, -d.stat().st_size if hasattr(d, \"stat\") else 0, d))\n",
    "    if not candidates:\n",
    "        return None\n",
    "    candidates.sort(key=lambda x: (x[0], x[1]), reverse=True)\n",
    "    return candidates[0][2]\n",
    "\n",
    "local_model_dir = find_local_hf_model()\n",
    "print(\"Local HF model:\", local_model_dir if local_model_dir else \"NOT FOUND (will fallback to sklearn)\")\n",
    "\n",
    "# ---------------- Option A: HF offline fine-tune ----------------\n",
    "def run_hf_offline(train_df, val_df, model_dir: Path):\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset\n",
    "    import transformers\n",
    "\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_dir, local_files_only=True\n",
    "    )\n",
    "\n",
    "    class TextClsDS(Dataset):\n",
    "        def __init__(self, df, tokenizer, max_len=256):\n",
    "            self.texts = df[\"review\"].tolist()\n",
    "            self.labels = df[\"label\"].tolist()\n",
    "            self.tok = tokenizer\n",
    "            self.max_len = max_len\n",
    "        def __len__(self): return len(self.texts)\n",
    "        def __getitem__(self, idx):\n",
    "            t = str(self.texts[idx])\n",
    "            enc = self.tok(\n",
    "                t, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\"\n",
    "            )\n",
    "            item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "            return item\n",
    "\n",
    "    train_ds = TextClsDS(train_df, tokenizer)\n",
    "    val_ds   = TextClsDS(val_df,   tokenizer)\n",
    "\n",
    "    # Build model with classification head (num_labels=2); load base weights from dir\n",
    "    model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_dir, num_labels=2, local_files_only=True\n",
    "    )\n",
    "\n",
    "    args = transformers.TrainingArguments(\n",
    "        output_dir=\"/kaggle/working/hf-out\",\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=1,        # keep quick\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        logging_steps=50,\n",
    "        report_to=[],\n",
    "    )\n",
    "\n",
    "    trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=lambda p: {\n",
    "            \"accuracy\": (p.predictions.argmax(axis=-1) == p.label_ids).mean().item()\n",
    "        },\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "    print(\"HF offline eval:\", metrics)\n",
    "    # Save model if you like\n",
    "    model.save_pretrained(\"/kaggle/working/hf-offline-model\")\n",
    "    tokenizer.save_pretrained(\"/kaggle/working/hf-offline-model\")\n",
    "\n",
    "# ---------------- Option B: Fallback sklearn baseline ----------------\n",
    "def run_sklearn_baseline(train_df, val_df):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(max_features=50000, ngram_range=(1,2))),\n",
    "        (\"clf\", LogisticRegression(max_iter=200))\n",
    "    ])\n",
    "    pipe.fit(train_df[\"review\"], train_df[\"label\"])\n",
    "    pred = pipe.predict(val_df[\"review\"])\n",
    "    acc = accuracy_score(val_df[\"label\"], pred)\n",
    "    print(\"Sklearn baseline accuracy:\", round(acc, 4))\n",
    "\n",
    "# ---------------- Run ----------------\n",
    "if local_model_dir:\n",
    "    try:\n",
    "        run_hf_offline(train_subset, val_subset, local_model_dir)\n",
    "    except Exception as e:\n",
    "        print(\"HF offline route failed, falling back to sklearn. Error:\", e)\n",
    "        run_sklearn_baseline(train_subset, val_subset)\n",
    "else:\n",
    "    run_sklearn_baseline(train_subset, val_subset)\n",
    "\n",
    "# Save splits for inspection\n",
    "out_dir = Path(\"/kaggle/working\")\n",
    "train_subset.to_csv(out_dir / \"train_subset.csv\", index=False)\n",
    "val_subset.to_csv(out_dir / \"val_subset.csv\", index=False)\n",
    "test_df.to_csv(out_dir / \"test_full_10pct.csv\", index=False)\n",
    "print(\"Saved splits to /kaggle/working/\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8220993,
     "sourceId": 12988208,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 92.578817,
   "end_time": "2025-09-07T17:06:07.785469",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-07T17:04:35.206652",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
