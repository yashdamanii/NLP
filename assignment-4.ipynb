{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9f13a83",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-07T17:34:48.893696Z",
     "iopub.status.busy": "2025-09-07T17:34:48.893271Z",
     "iopub.status.idle": "2025-09-07T17:34:53.022983Z",
     "shell.execute_reply": "2025-09-07T17:34:53.021738Z"
    },
    "papermill": {
     "duration": 4.135682,
     "end_time": "2025-09-07T17:34:53.024262",
     "exception": true,
     "start_time": "2025-09-07T17:34:48.888580",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fuzzy lib: difflib\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'resolved_queries.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13/1282695478.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# ---------------- Load data ----------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mresolved_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'resolved_queries.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mnew_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'new_queries.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'resolved_queries.csv'"
     ]
    }
   ],
   "source": [
    "# SINGLE-CELL: Fuzzy + TF-IDF matching with robust imports and fallbacks\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ---------------- Imports with fallbacks ----------------\n",
    "FUZZ_LIB = None\n",
    "try:\n",
    "    from thefuzz import fuzz, process\n",
    "    FUZZ_LIB = 'thefuzz'\n",
    "except Exception:\n",
    "    try:\n",
    "        from rapidfuzz import fuzz, process\n",
    "        FUZZ_LIB = 'rapidfuzz'\n",
    "    except Exception:\n",
    "        import difflib\n",
    "        FUZZ_LIB = 'difflib'\n",
    "\n",
    "# NLTK stopwords (download if missing)\n",
    "try:\n",
    "    from nltk.corpus import stopwords\n",
    "    _ = stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "print(f\"Using fuzzy lib: {FUZZ_LIB}\")\n",
    "\n",
    "# ---------------- Load data ----------------\n",
    "resolved_df = pd.read_csv('resolved_queries.csv')\n",
    "new_df = pd.read_csv('new_queries.csv')\n",
    "\n",
    "# Normalize expected column names (adjust here if your CSV uses different names)\n",
    "# Required:\n",
    "#   resolved_df: 'Query_ID', 'Pre_Resolved_Query'\n",
    "#   new_df:      'Variation_Query', 'Matches_With_Query_ID'\n",
    "for col in ['Query_ID', 'Pre_Resolved_Query']:\n",
    "    assert col in resolved_df.columns, f\"Column '{col}' missing in resolved_queries.csv\"\n",
    "for col in ['Variation_Query', 'Matches_With_Query_ID']:\n",
    "    assert col in new_df.columns, f\"Column '{col}' missing in new_queries.csv\"\n",
    "\n",
    "# Basic cleanup\n",
    "for c in ['Pre_Resolved_Query']:\n",
    "    resolved_df[c] = resolved_df[c].fillna('')\n",
    "for c in ['Variation_Query']:\n",
    "    new_df[c] = new_df[c].fillna('')\n",
    "\n",
    "print(\"Resolved queries columns:\", resolved_df.columns.tolist())\n",
    "print(\"New queries columns:\", new_df.columns.tolist())\n",
    "\n",
    "# ---------------- Preprocessing ----------------\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [stemmer.stem(tok) for tok in tokens if tok not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "resolved_df['processed'] = resolved_df['Pre_Resolved_Query'].apply(preprocess)\n",
    "new_df['processed'] = new_df['Variation_Query'].apply(preprocess)\n",
    "\n",
    "# Build IDs & lookups\n",
    "id_to_processed = dict(zip(resolved_df['Query_ID'], resolved_df['processed']))\n",
    "id_to_original  = dict(zip(resolved_df['Query_ID'], resolved_df['Pre_Resolved_Query']))\n",
    "\n",
    "# Note: multiple different Query_IDs can map to the same processed string.\n",
    "# Keep a list to avoid collisions.\n",
    "from collections import defaultdict as dd\n",
    "processed_to_ids = dd(list)\n",
    "for qid, proc in zip(resolved_df['Query_ID'], resolved_df['processed']):\n",
    "    processed_to_ids[proc].append(qid)\n",
    "\n",
    "resolved_processed_list = resolved_df['processed'].tolist()\n",
    "\n",
    "# ---------------- TF-IDF Vectorizer ----------------\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(resolved_df['processed'])\n",
    "\n",
    "# ---------------- Fuzzy helpers ----------------\n",
    "def _extract_one_difflib(query, choices):\n",
    "    # Returns (match, score) with 0-100 score\n",
    "    if not choices:\n",
    "        return None, 0\n",
    "    best = None\n",
    "    best_score = -1\n",
    "    for c in choices:\n",
    "        score = int(100 * (difflib.SequenceMatcher(None, query, c).ratio()))\n",
    "        if score > best_score:\n",
    "            best = c\n",
    "            best_score = score\n",
    "    return best, best_score\n",
    "\n",
    "def get_fuzzy_match(query, choices, threshold=85):\n",
    "    \"\"\"\n",
    "    Try several fuzzy scorers (if available), pick the best.\n",
    "    Returns (best_match_string, best_score, method_name) or (None, score, method)\n",
    "    \"\"\"\n",
    "    if FUZZ_LIB in ('thefuzz', 'rapidfuzz'):\n",
    "        methods = [\n",
    "            ('Simple Ratio', getattr(fuzz, 'ratio')),\n",
    "            ('Partial Ratio', getattr(fuzz, 'partial_ratio', getattr(fuzz, 'QRatio', getattr(fuzz, 'WRatio', None)))),\n",
    "            ('Token Sort Ratio', getattr(fuzz, 'token_sort_ratio', getattr(fuzz, 'ratio'))),\n",
    "            ('Token Set Ratio', getattr(fuzz, 'token_set_ratio', getattr(fuzz, 'ratio'))),\n",
    "            ('Partial Token Sort Ratio', getattr(fuzz, 'partial_token_sort_ratio', getattr(fuzz, 'ratio')))\n",
    "        ]\n",
    "        best_match, best_score, best_method = None, 0, \"\"\n",
    "        for method_name, scorer in methods:\n",
    "            if scorer is None:\n",
    "                continue\n",
    "            # both thefuzz and rapidfuzz expose process.extractOne with scorer=\n",
    "            match = process.extractOne(query, choices, scorer=scorer)\n",
    "            if match:\n",
    "                candidate, score = match[0], match[1]\n",
    "                if score > best_score:\n",
    "                    best_match, best_score, best_method = candidate, score, method_name\n",
    "        if best_score >= threshold:\n",
    "            return best_match, int(best_score), best_method\n",
    "        return None, int(best_score), best_method\n",
    "    else:\n",
    "        # difflib fallback\n",
    "        match, score = _extract_one_difflib(query, choices)\n",
    "        method = \"difflib SequenceMatcher\"\n",
    "        if score >= threshold:\n",
    "            return match, int(score), method\n",
    "        return None, int(score), method\n",
    "\n",
    "# ---------------- Cosine helper ----------------\n",
    "def get_cosine_match(query, tfidf_matrix, vectorizer, threshold=0.7):\n",
    "    qv = vectorizer.transform([query])\n",
    "    cos = cosine_similarity(qv, tfidf_matrix).flatten()\n",
    "    idx = int(cos.argmax())\n",
    "    score = float(cos[idx])\n",
    "    if score >= threshold:\n",
    "        return resolved_processed_list[idx], score\n",
    "    return None, score\n",
    "\n",
    "# ---------------- Matching loop ----------------\n",
    "results = []\n",
    "method_performance = defaultdict(lambda: {'correct': 0, 'total': 0, 'scores': []})\n",
    "\n",
    "for _, row in new_df.iterrows():\n",
    "    new_query_proc = row['processed']\n",
    "    original_query = row['Variation_Query']\n",
    "    ground_truth_id = row['Matches_With_Query_ID']\n",
    "\n",
    "    # Get ground-truth processed (may be empty if ID missing)\n",
    "    gt_processed = id_to_processed.get(ground_truth_id, \"\")\n",
    "\n",
    "    # Fuzzy\n",
    "    fuzzy_match, fuzzy_score, fuzzy_method = get_fuzzy_match(\n",
    "        new_query_proc, resolved_processed_list, threshold=85\n",
    "    )\n",
    "    # Map fuzzy string -> candidate IDs (may be multiple)\n",
    "    fuzzy_predicted_ids = processed_to_ids.get(fuzzy_match, []) if fuzzy_match else []\n",
    "    # If multiple, accept correct if any matches GT\n",
    "    fuzzy_correct = ground_truth_id in fuzzy_predicted_ids if fuzzy_predicted_ids else False\n",
    "    fuzzy_predicted_id = fuzzy_predicted_ids[0] if fuzzy_predicted_ids else None\n",
    "\n",
    "    # Cosine\n",
    "    cosine_match, cosine_score = get_cosine_match(\n",
    "        new_query_proc, tfidf_matrix, vectorizer, threshold=0.7\n",
    "    )\n",
    "    cosine_predicted_ids = processed_to_ids.get(cosine_match, []) if cosine_match else []\n",
    "    cosine_correct = ground_truth_id in cosine_predicted_ids if cosine_predicted_ids else False\n",
    "    cosine_predicted_id = cosine_predicted_ids[0] if cosine_predicted_ids else None\n",
    "\n",
    "    # Track per-method stats\n",
    "    if fuzzy_method:\n",
    "        method_performance[fuzzy_method]['correct'] += 1 if fuzzy_correct else 0\n",
    "        method_performance[fuzzy_method]['total'] += 1\n",
    "        method_performance[fuzzy_method]['scores'].append(fuzzy_score)\n",
    "\n",
    "    method_performance['TF-IDF Cosine']['correct'] += 1 if cosine_correct else 0\n",
    "    method_performance['TF-IDF Cosine']['total'] += 1\n",
    "    method_performance['TF-IDF Cosine']['scores'].append(cosine_score)\n",
    "\n",
    "    # Store row\n",
    "    results.append({\n",
    "        'Variation_Query': original_query,\n",
    "        'Ground_Truth_ID': ground_truth_id,\n",
    "        'Ground_Truth_Query': id_to_original.get(ground_truth_id, \"\"),\n",
    "        'Fuzzy_Predicted_ID': fuzzy_predicted_id,\n",
    "        'Fuzzy_Predicted_Query': id_to_original.get(fuzzy_predicted_id, \"\") if fuzzy_predicted_id else \"\",\n",
    "        'Fuzzy_Method': fuzzy_method if fuzzy_match else \"None\",\n",
    "        'Fuzzy_Score': fuzzy_score,\n",
    "        'Fuzzy_Correct': fuzzy_correct,\n",
    "        'TFIDF_Predicted_ID': cosine_predicted_id,\n",
    "        'TFIDF_Predicted_Query': id_to_original.get(cosine_predicted_id, \"\") if cosine_predicted_id else \"\",\n",
    "        'TFIDF_Score': cosine_score,\n",
    "        'TFIDF_Correct': cosine_correct\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# ---------------- Aggregate metrics ----------------\n",
    "performance_data = []\n",
    "for method, stats in method_performance.items():\n",
    "    total = stats['total']\n",
    "    if total > 0:\n",
    "        accuracy = stats['correct'] / total\n",
    "        avg_score = float(np.mean(stats['scores'])) if stats['scores'] else 0.0\n",
    "        performance_data.append({\n",
    "            'Method': method,\n",
    "            'Accuracy': accuracy,\n",
    "            'Coverage': total / len(new_df) if len(new_df) else 0.0,\n",
    "            'Average_Score': avg_score,\n",
    "            'Correct_Matches': stats['correct'],\n",
    "            'Total_Matches': total\n",
    "        })\n",
    "performance_df = pd.DataFrame(performance_data).sort_values(['Accuracy','Coverage'], ascending=[False, False])\n",
    "\n",
    "# ---------------- Save & print ----------------\n",
    "results_df.to_csv('matched_queries_results_individual.csv', index=False)\n",
    "performance_df.to_csv('method_performance_comparison_individual.csv', index=False)\n",
    "\n",
    "print(f\"\\nProcessed {len(new_df)} queries\")\n",
    "\n",
    "print(\"\\nMethod Performance Comparison (Individual Methods Only):\")\n",
    "print(performance_df)\n",
    "\n",
    "print(\"\\nSample matches:\")\n",
    "cols = ['Variation_Query', 'Ground_Truth_Query',\n",
    "        'Fuzzy_Predicted_Query', 'Fuzzy_Method', 'Fuzzy_Correct',\n",
    "        'TFIDF_Predicted_Query', 'TFIDF_Correct']\n",
    "print(results_df[cols].head(10).to_string(index=False))\n",
    "\n",
    "# Detailed per-method summary\n",
    "print(\"\\n\\nDetailed analysis by method:\")\n",
    "for method in performance_df['Method'].unique():\n",
    "    row = performance_df[performance_df['Method'] == method].iloc[0]\n",
    "    print(f\"\\n{method}:\")\n",
    "    print(f\"  Accuracy: {row['Accuracy']:.2%}\")\n",
    "    print(f\"  Coverage: {row['Coverage']:.2%}\")\n",
    "    print(f\"  Average Score: {row['Average_Score']:.2f}\")\n",
    "    print(f\"  Correct Matches: {int(row['Correct_Matches'])}/{int(row['Total_Matches'])}\")\n",
    "\n",
    "# Best method\n",
    "if not performance_df.empty:\n",
    "    best = performance_df.iloc[0]\n",
    "    print(f\"\\nBest individual method: {best['Method']} \"\n",
    "          f\"with accuracy {best['Accuracy']:.2%} and coverage {best['Coverage']:.2%}\")\n",
    "\n",
    "# Combined stats\n",
    "fuzzy_methods = [m for m in performance_df['Method'] if m != 'TF-IDF Cosine']\n",
    "fuzzy_stats = performance_df[performance_df['Method'].isin(fuzzy_methods)]\n",
    "if not fuzzy_stats.empty:\n",
    "    total_correct = int(fuzzy_stats['Correct_Matches'].sum())\n",
    "    total_attempts = int(fuzzy_stats['Total_Matches'].sum())\n",
    "    overall_acc = total_correct / total_attempts if total_attempts else 0.0\n",
    "    overall_cov = total_attempts / len(new_df) if len(new_df) else 0.0\n",
    "    print(f\"\\nFuzzy Methods Combined:\\n  Total Correct: {total_correct}\\n  Total Matches: {total_attempts}\"\n",
    "          f\"\\n  Overall Accuracy: {overall_acc:.2%}\\n  Overall Coverage: {overall_cov:.2%}\")\n",
    "\n",
    "tfidf_stats = performance_df[performance_df['Method'] == 'TF-IDF Cosine']\n",
    "if not tfidf_stats.empty:\n",
    "    t = tfidf_stats.iloc[0]\n",
    "    print(f\"\\nTF-IDF Cosine:\\n  Correct Matches: {int(t['Correct_Matches'])}\"\n",
    "          f\"\\n  Total Matches: {int(t['Total_Matches'])}\"\n",
    "          f\"\\n  Accuracy: {t['Accuracy']:.2%}\\n  Coverage: {t['Coverage']:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aa78f8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8221164,
     "sourceId": 12988450,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8221167,
     "sourceId": 12988453,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8221257,
     "sourceId": 12988575,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8221260,
     "sourceId": 12988578,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.677226,
   "end_time": "2025-09-07T17:34:53.648235",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-07T17:34:43.971009",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
